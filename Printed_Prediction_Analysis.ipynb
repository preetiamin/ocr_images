{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model.helper import *\n",
    "from model.train_ner import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook processes raw OCR Text output from post 1990 images and analyzes accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw data with OCR text/barcodes, labeled data, nlp model and regex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df = pd.read_excel('data/printed_raw_ocr_data.xlsx')\n",
    "mod = printed_model()\n",
    "df_labels = pd.read_excel('data/specimens_post_1990.xlsx')\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions using the regex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 5.5 secs\n"
     ]
    }
   ],
   "source": [
    "# Retrieve entities using regex\n",
    "starttime = time.time()\n",
    "df['Clean_text'] = df['OCRText'].apply(mod.clean_text)\n",
    "df['County'] = df['Clean_text'].apply(mod.find_county)\n",
    "df['State'] = df['Clean_text'].apply(mod.find_state)\n",
    "df['Species'] = df['Clean_text'].apply(mod.find_species)\n",
    "df['Date'] = df['Clean_text'].apply(mod.find_date)\n",
    "df['Collector'] = df['Clean_text'].apply(mod.find_collector)\n",
    "print('Total time: {:.1f} secs'.format(time.time()-starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions using the nlp model: County, State and Species are predicted using a custom NER model, Date and Collector with general 'en' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 239.6 secs\n"
     ]
    }
   ],
   "source": [
    "# Predict entities using nlp\n",
    "starttime = time.time()\n",
    "df['Pred_County'] = df['Clean_text'].dropna().apply(mod.predict_county)\n",
    "df['Pred_State'] = df['Clean_text'].dropna().apply(mod.predict_state)\n",
    "df['Pred_Species'] = df['Clean_text'].dropna().apply(mod.predict_species)\n",
    "df['Pred_Date'] = df['Clean_text'].dropna().apply(mod.predict_date)\n",
    "df['Pred_Collector'] = df['Clean_text'].dropna().apply(mod.predict_collector)\n",
    "print('Total time: {:.1f} secs'.format(time.time()-starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pred_State'] = df.apply(lambda x:x['State'] if x['Pred_State'] is None else x['Pred_State'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load labels from labeled data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.0 secs\n"
     ]
    }
   ],
   "source": [
    "# Get ground truth labels\n",
    "starttime = time.time()\n",
    "df['Act_Barcode'] = df_labels['ColBarcode']\n",
    "df['Act_County'] = df_labels['RDECounty']\n",
    "df['Act_State'] = df_labels['RDEProvinceState']\n",
    "df['Act_Date'] = df_labels['RDEDateFrom']\n",
    "df['Act_Collector1'] = df_labels['NamBriefName']\n",
    "df['Act_Collector2'] = df_labels['RDECollectionTeam']\n",
    "df['Act_County'] = df['Act_County'].dropna().apply(lambda x:' '.join(x.split()[:-1]))\n",
    "print('Total time: {:.1f} secs'.format(time.time()-starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change dates to date formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df['Pred_Date'] = pd.to_datetime(df['Pred_Date'], errors='coerce')\n",
    "df['Act_Date'] = pd.to_datetime(df['Act_Date'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df):\n",
    "    reg_acc_state = 100 * df[df['State']==df['Act_State']].shape[0]/df.shape[0]\n",
    "    reg_acc_county = 100 * df[df['County']==df['Act_County']].shape[0]/df.shape[0]\n",
    "    reg_acc_date = 100 * df[df['Date']==df['Act_Date']].shape[0]/df.shape[0]\n",
    "    reg_perc_species = 100 * df['Species'].count()/df.shape[0]\n",
    "\n",
    "\n",
    "    nlp_acc_state = 100 * df[df['Pred_State']==df['Act_State']].shape[0]/df.shape[0]\n",
    "    nlp_acc_county = 100 * df[df['Pred_County']==df['Act_County']].shape[0]/df.shape[0]\n",
    "    nlp_acc_date = 100 * df[df['Pred_Date']==df['Act_Date']].shape[0]/df.shape[0]\n",
    "    nlp_perc_species = 100 * df['Pred_Species'].count()/df.shape[0]\n",
    "\n",
    "    acc_barcode = 100 * df[df['Barcode']==df['Act_Barcode']].shape[0]/df.shape[0]\n",
    "    \n",
    "    acc_overall = 100 * df[(df['Pred_State']==df['Act_State']) &\n",
    "                           (df['Pred_County']==df['Act_County']) &\n",
    "                           (df['Pred_Species'].notnull()) &\n",
    "                           (df['Barcode']==df['Act_Barcode']) &\n",
    "                           (df['Date']==df['Act_Date'])].shape[0]/df.shape[0]\n",
    "\n",
    "    print('Regex Accuracy ======> State: {:.1f}%, County: {:.1f}%, Date: {:.1f}%, Species predicted: {:.1f}%'.format(\n",
    "        reg_acc_state,reg_acc_county, reg_acc_date, reg_perc_species))\n",
    "    print('NLP Accuracy ======> State: {:.1f}%, County: {:.1f}%, Date: {:.1f}%, Species predicted: {:.1f}%'.format(\n",
    "        nlp_acc_state, nlp_acc_county, nlp_acc_date, nlp_perc_species))\n",
    "    print('Barcode accuracy ======> {:.1f}%'.format(acc_barcode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict accuracy with both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex Accuracy ======> State: 94.6%, County: 90.8%, Date: 94.5%, Species predicted: 86.7%\n",
      "NLP Accuracy ======> State: 98.4%, County: 96.0%, Date: 57.7%, Species predicted: 95.9%\n",
      "Barcode accuracy ======> 98.9%\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove rows where either State or County is not in OCRText, most of these are manual entry errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(x):\n",
    "    if x.OCRText:\n",
    "        if x.Act_County:\n",
    "            if x.Act_County.lower() not in x.OCRText.lower():\n",
    "                return 'County Mislabeled'\n",
    "        if x.Act_State:\n",
    "            if x.Act_State.lower() not in x.OCRText.lower():\n",
    "                return 'State Mislabeled'\n",
    "    return None\n",
    "    \n",
    "df['Error'] = df.dropna(subset=['Act_State', 'Act_County', 'OCRText']).apply(find_errors, axis=1)\n",
    "dfError = df[df['Error'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mismatches(x):\n",
    "    if x.OCRText:\n",
    "        if (x.Act_County) and (x.Pred_County):\n",
    "            if x.Pred_County.lower() != x.Act_County.lower():\n",
    "                return 'County Mismatched'\n",
    "        if (x.Act_State) and (x.Pred_State):\n",
    "            if x.Pred_State.lower() != x.Act_State.lower():\n",
    "                return 'State Mismatched'\n",
    "    return None\n",
    "    \n",
    "df['Mismatch'] = df.dropna(subset=['Act_State', 'Act_County', 'OCRText']).apply(find_mismatches, axis=1)\n",
    "dfMismatch = df[df['Mismatch'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors/mismatches were manually evaluated for State and County and errors originating from specimen label or label entry errors were saved in a file, we'll remove this from our evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Barcode</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>Date</th>\n",
       "      <th>Collector</th>\n",
       "      <th>Collector 2</th>\n",
       "      <th>Error</th>\n",
       "      <th>Error Description</th>\n",
       "      <th>Pred Made?</th>\n",
       "      <th>Pred Right?</th>\n",
       "      <th>Close Match?</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3339039</td>\n",
       "      <td>Grand</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8/5/04</td>\n",
       "      <td>E. Foley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>County Mislabeled</td>\n",
       "      <td>Label wrong (should be Clear Creek)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Label Entry Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3234787</td>\n",
       "      <td>Grand</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8/16/04</td>\n",
       "      <td>E. Foley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>County Mislabeled</td>\n",
       "      <td>Label wrong (should be Clear Creek)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Label Entry Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3261965</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8/14/96</td>\n",
       "      <td>D. Atwood</td>\n",
       "      <td>NaN</td>\n",
       "      <td>County Mislabeled</td>\n",
       "      <td>Label wrong (should be San Miguel)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Label Entry Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3134482</td>\n",
       "      <td>Nassau</td>\n",
       "      <td>New York</td>\n",
       "      <td>5/8/12</td>\n",
       "      <td>M. Bennett</td>\n",
       "      <td>Gleason &amp; Cronquist</td>\n",
       "      <td>County Mislabeled</td>\n",
       "      <td>Label wrong (no county listed)</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Specimen Label Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3134483</td>\n",
       "      <td>Nassau</td>\n",
       "      <td>New York</td>\n",
       "      <td>5/8/12</td>\n",
       "      <td>M. Bennett</td>\n",
       "      <td>Gleason &amp; Cronquist</td>\n",
       "      <td>County Mislabeled</td>\n",
       "      <td>Label wrong (no county listed)</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Specimen Label Error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Barcode    County     State     Date   Collector          Collector 2  \\\n",
       "0  3339039     Grand  Colorado   8/5/04    E. Foley                  NaN   \n",
       "1  3234787     Grand  Colorado  8/16/04    E. Foley                  NaN   \n",
       "2  3261965  Saguache  Colorado  8/14/96   D. Atwood                  NaN   \n",
       "3  3134482    Nassau  New York   5/8/12  M. Bennett  Gleason & Cronquist   \n",
       "4  3134483    Nassau  New York   5/8/12  M. Bennett  Gleason & Cronquist   \n",
       "\n",
       "               Error                    Error Description Pred Made?  \\\n",
       "0  County Mislabeled  Label wrong (should be Clear Creek)        Yes   \n",
       "1  County Mislabeled  Label wrong (should be Clear Creek)        Yes   \n",
       "2  County Mislabeled   Label wrong (should be San Miguel)        Yes   \n",
       "3  County Mislabeled       Label wrong (no county listed)         No   \n",
       "4  County Mislabeled       Label wrong (no county listed)         No   \n",
       "\n",
       "  Pred Right? Close Match?              Category  \n",
       "0         Yes           No     Label Entry Error  \n",
       "1         Yes           No     Label Entry Error  \n",
       "2         Yes           No     Label Entry Error  \n",
       "3         Yes           No  Specimen Label Error  \n",
       "4         Yes           No  Specimen Label Error  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_errors = pd.read_csv('data/printed_label_errors.csv')\n",
    "df_data_errors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove these from the dataframe and calculate accuracy again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex Accuracy ======> State: 95.1%, County: 91.8%, Date: 94.7%, Species predicted: 87.0%\n",
      "NLP Accuracy ======> State: 99.0%, County: 97.1%, Date: 57.8%, Species predicted: 96.2%\n",
      "Barcode accuracy ======> 99.0%\n"
     ]
    }
   ],
   "source": [
    "df = df[~df.Act_Barcode.isin(df_data_errors.Barcode)]\n",
    "calculate_accuracy(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/printed_prediction_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NLP model for State, County and Species, and Regex model for Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(df, pred_col,act_col):\n",
    "    TP = df[df[pred_col]==df[act_col]].shape[0]\n",
    "    TP_FP = df[pred_col].notnull().sum()\n",
    "    TP_FN = df[act_col].notnull().sum()\n",
    "    Precision = TP/TP_FP\n",
    "    Recall = TP/TP_FN\n",
    "    F1Score = (2 * Precision * Recall) / (Precision + Recall)\n",
    "    return [pred_col, Precision, Recall, F1Score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate precision, recall and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Pred_State</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Pred_County</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Barcode</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Date</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entity  Precision  Recall  F1Score\n",
       "0   Pred_State      0.991   0.990    0.991\n",
       "1  Pred_County      0.977   0.972    0.974\n",
       "2      Barcode      0.992   0.990    0.991\n",
       "3         Date      0.990   0.948    0.968"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfeval = pd.DataFrame()\n",
    "dfeval = dfeval.append([calc_metrics(df, 'Pred_State', 'Act_State')],ignore_index=True)\n",
    "dfeval = dfeval.append([calc_metrics(df, 'Pred_County', 'Act_County')],ignore_index=True)\n",
    "dfeval = dfeval.append([calc_metrics(df, 'Barcode', 'Act_Barcode')],ignore_index=True)\n",
    "dfeval = dfeval.append([calc_metrics(df, 'Date', 'Act_Date')],ignore_index=True)\n",
    "dfeval.columns = ['Entity', 'Precision', 'Recall', 'F1Score']\n",
    "dfeval.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate scores using spacy scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/printed_prediction_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found errors in data, see data_errors.txt\n"
     ]
    }
   ],
   "source": [
    "X = df['OCRText']\n",
    "Y = df[['Pred_State','Pred_County','Pred_Species']]\n",
    "Y.rename(columns={'Pred_State': 'State', 'Pred_County': 'County',\n",
    "                  'Pred_Species': 'Species'}, inplace=True)\n",
    "test_data, missing_data, overlap_data = gen_data(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4917"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = printed_model()\n",
    "score = evaluate_model(mod.nlp_cus,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Species': {'p': 96.73257023933402, 'r': 99.55022488755623, 'f': 98.12117373865316}, 'State': {'p': 97.82202862476665, 'r': 97.45815251084935, 'f': 97.63975155279503}, 'County': {'p': 98.53151131960024, 'r': 99.6493399339934, 'f': 99.08727310019485}}\n"
     ]
    }
   ],
   "source": [
    "print(score['ents_per_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores on states look slightly worse here than calculations above; this makes sense becuase we applied regex model after the predicted model to increase the prediction rate\n",
    "\n",
    "For our evaluation, we'll use the scores that we calculated above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
